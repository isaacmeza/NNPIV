{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# DML (mediated) with Neural Nets — AGMM (sequential) & AGMM2L2 (simultaneous)\n",
    "# =========================================================\n",
    "\n",
    "# ---- Limit BLAS/OpenMP threads BEFORE importing heavy libs ----\n",
    "import os as os\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# ---- Standard libs ----\n",
    "import sys\n",
    "import time\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Third-party ----\n",
    "import numpy as np\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# Keep native libraries (BLAS/OpenMP) to 1 thread\n",
    "threadpool_limits(1)\n",
    "\n",
    "# ---- Local repo imports (adjust path if needed) ----\n",
    "sys.path.append(str(Path.cwd() / \"../../simulations\"))\n",
    "import dgps_mediated as dgps  \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn  \n",
    "from nnpiv.neuralnet.agmm import AGMM  \n",
    "from nnpiv.neuralnet.agmm2 import AGMM2L2 \n",
    "from nnpiv.semiparametrics import DML_mediated  \n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Reproducibility helpers\n",
    "# -----------------------\n",
    "def seed_everything(seed: int = 123) -> None:\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Keep torch to 1 thread too\n",
    "    try:\n",
    "        torch.set_num_threads(1)\n",
    "        torch.set_num_interop_threads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seed_everything(123)\n",
    "\n",
    "# -----------------------\n",
    "# Resource print utility\n",
    "# -----------------------\n",
    "def print_resources():\n",
    "    \"\"\"Print basic compute resource info (CPU, GPU, library versions).\"\"\"\n",
    "    cpu_cores = os.cpu_count()\n",
    "    pyver = sys.version.split()[0]\n",
    "    npver = np.__version__\n",
    "    torchver = torch.__version__\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "        except Exception:\n",
    "            gpu_name = \"Unknown GPU\"\n",
    "        gpu_info = f\"CUDA: available — {gpu_name}\"\n",
    "    else:\n",
    "        gpu_info = \"CUDA: not available\"\n",
    "    print(\"=== Compute resources ===\")\n",
    "    print(f\"Python: {pyver}\")\n",
    "    print(f\"NumPy: {npver}\")\n",
    "    print(f\"PyTorch: {torchver}\")\n",
    "    print(f\"CPU cores: {cpu_cores}\")\n",
    "    print(gpu_info)\n",
    "    print(\"Thread caps (env):\")\n",
    "    for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\n",
    "              \"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
    "        print(f\"  {k}={os.environ.get(k, 'unset')}\")\n",
    "    print(f\"Platform: {platform.platform()}\")\n",
    "    print(\"=========================\\n\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Result formatter\n",
    "# -----------------------\n",
    "def summarize_dml_result(name: str, result, elapsed: float):\n",
    "    \"\"\"\n",
    "    Accepts result from .dml() and prints θ, SE, 95% CI when available.\n",
    "    Compatible with returns like (theta, var, ci) or (theta, var, ci, cov).\n",
    "    \"\"\"\n",
    "    if isinstance(result, tuple):\n",
    "        if len(result) == 3:\n",
    "            theta, var, ci = result\n",
    "            cov = None\n",
    "        elif len(result) == 4:\n",
    "            theta, var, ci, cov = result\n",
    "        else:\n",
    "            print(f\"[{name}] time={elapsed:.2f}s — result={result}\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"[{name}] time={elapsed:.2f}s — result={result}\")\n",
    "        return\n",
    "\n",
    "    theta = np.atleast_1d(theta).astype(float)\n",
    "    var = np.atleast_1d(var).astype(float)\n",
    "    se = np.sqrt(var)\n",
    "    ci = np.array(ci, dtype=float) if ci is not None else None\n",
    "\n",
    "    def fmt_arr(a):\n",
    "        return f\"{float(a[0]):.4f}\" if a.size == 1 else np.array2string(a, precision=4)\n",
    "\n",
    "    print(f\"[{name}] time={elapsed:.2f}s\")\n",
    "    print(f\"  theta: {fmt_arr(theta)}\")\n",
    "    print(f\"  SE   : {fmt_arr(se)}\")\n",
    "    if ci is not None:\n",
    "        if ci.ndim == 1 and ci.size == 2:\n",
    "            print(f\"  95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "        else:\n",
    "            print(f\"  95% CI: {np.array2string(ci, precision=4)}\")\n",
    "    if 'cov' in locals() and cov is not None:\n",
    "        print(f\"  (cov shape: {cov.shape})\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Compute resources ===\n",
      "Python: 3.10.18\n",
      "NumPy: 2.2.6\n",
      "PyTorch: 2.5.0\n",
      "CPU cores: 112\n",
      "CUDA: not available\n",
      "Thread caps (env):\n",
      "  OMP_NUM_THREADS=1\n",
      "  OPENBLAS_NUM_THREADS=1\n",
      "  MKL_NUM_THREADS=1\n",
      "  VECLIB_MAXIMUM_THREADS=1\n",
      "  NUMEXPR_NUM_THREADS=1\n",
      "Platform: Linux-4.18.0-553.44.1.el8_10.x86_64-x86_64-with-glibc2.28\n",
      "=========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Print resources \n",
    "# -----------------------\n",
    "print_resources()\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ground truth (for log reference) ===\n",
      "True parameter for E[Y(1,M(0))] ≈ 4.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Data generation\n",
    "# =========================================================\n",
    "# Function dictionary (for reference):\n",
    "# {'abs': 0, '2dpoly': 1, 'sigmoid': 2,\n",
    "#  'sin': 3, 'frequent_sin': 4, 'abs_sqrt': 5, 'step': 6, '3dpoly': 7,\n",
    "#  'linear': 8, 'rand_pw': 9, 'abspos': 10, 'sqrpos': 11, 'band': 12,\n",
    "#  'invband': 13, 'steplinear': 14, 'pwlinear': 15, 'exponential': 16}\n",
    "\n",
    "fn_number = 0\n",
    "tau_fn = dgps.get_tau_fn(fn_number)\n",
    "tauinv_fn = dgps.get_tauinv_fn(fn_number)  # kept for parity with your code\n",
    "W, Z, X, M, D, Y, tau_fn = dgps.get_data(2000, tau_fn)\n",
    "\n",
    "# Ground-truth value for the target estimand (for log reference)\n",
    "TRUE_PARAM = 4.05\n",
    "print(f\"=== Ground truth (for log reference) ===\\nTrue parameter for E[Y(1,M(0))] ≈ {TRUE_PARAM:.2f}\\n\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# NN architecture helpers (dropout & width are configurable)\n",
    "# =========================================================\n",
    "p = 0.10\n",
    "n_hidden = 100\n",
    "\n",
    "def _get_learner(n_t: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Dropout(p=p), nn.Linear(n_t, n_hidden), nn.LeakyReLU(),\n",
    "        nn.Dropout(p=p), nn.Linear(n_hidden, 1)\n",
    "    )\n",
    "\n",
    "def _get_adversary(n_z: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Dropout(p=p), nn.Linear(n_z, n_hidden), nn.LeakyReLU(),\n",
    "        nn.Dropout(p=p), nn.Linear(n_hidden, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Model builders (dimensions inferred from data)\n",
    "# =========================================================\n",
    "def build_agmm_pair_for_mediated(M, X, W, Z):\n",
    "    \"\"\"\n",
    "    Build two AGMM models with correct input dims for the mediated setup.\n",
    "    Stage 1 (bridge on treated arm):\n",
    "        T = [M, X, W], Z = [M, X, Z]\n",
    "    Stage 2:\n",
    "        T = [X, W],     Z = [X, Z]\n",
    "    \"\"\"\n",
    "    T1_dim = M.shape[1] + X.shape[1] + W.shape[1]\n",
    "    Z1_dim = M.shape[1] + X.shape[1] + Z.shape[1]\n",
    "    T2_dim = X.shape[1] + W.shape[1]\n",
    "    Z2_dim = X.shape[1] + Z.shape[1]\n",
    "    m1 = AGMM(_get_learner(T1_dim), _get_adversary(Z1_dim))\n",
    "    m2 = AGMM(_get_learner(T2_dim), _get_adversary(Z2_dim))\n",
    "    return m1, m2\n",
    "\n",
    "def build_agmm2_for_mediated(M, X, W, Z):\n",
    "    \"\"\"For model1 (outcome bridge).\"\"\"\n",
    "    A_dim = M.shape[1] + X.shape[1] + W.shape[1]   \n",
    "    B_dim = X.shape[1] + W.shape[1]                 \n",
    "    E_dim = M.shape[1] + X.shape[1] + Z.shape[1]   \n",
    "    C_dim = X.shape[1] + Z.shape[1]                 \n",
    "    return AGMM2L2(\n",
    "        learnerh=_get_learner(B_dim),\n",
    "        learnerg=_get_learner(A_dim),\n",
    "        adversary1=_get_adversary(E_dim),\n",
    "        adversary2=_get_adversary(C_dim),\n",
    "    )\n",
    "\n",
    "def build_agmm2_for_mediated_q1(M, X, W, Z):\n",
    "    \"\"\"For model_q1 (q-bridge).\"\"\"\n",
    "    A_prime_dim = X.shape[1] + W.shape[1]                 #  (this goes to learnerg)\n",
    "    B_prime_dim = M.shape[1] + X.shape[1] + W.shape[1]    #  (this goes to learnerh)\n",
    "    D_prime_dim = X.shape[1] + Z.shape[1]                 #  (this goes to adversary1)\n",
    "    C_prime_dim = M.shape[1] + X.shape[1] + Z.shape[1]    #  (this goes to adversary2)\n",
    "    return AGMM2L2(\n",
    "        learnerh=_get_learner(B_prime_dim),   \n",
    "        learnerg=_get_learner(A_prime_dim),   \n",
    "        adversary1=_get_adversary(D_prime_dim),  \n",
    "        adversary2=_get_adversary(C_prime_dim),  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:17<00:00, 39.59s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential (MR) with AGMM] time=197.93s\n",
      "  theta: 4.0745\n",
      "  SE   : 5.2253\n",
      "  95% CI: [3.8455, 4.3035]\n",
      "\n",
      "Rep: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:24<00:00, 136.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Simultaneous (MR) with AGMM2L2] time=684.06s\n",
      "  theta: 4.1246\n",
      "  SE   : 5.2737\n",
      "  95% CI: [3.8935, 4.3557]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1) Sequential estimator (MR) with AGMM\n",
    "# =========================================================\n",
    "m1, m2 = build_agmm_pair_for_mediated(M, X, W, Z)\n",
    "fitargs_seq = {\n",
    "    \"n_epochs\": 300, \"bs\": 100,\n",
    "    \"learner_lr\": 1e-4, \"adversary_lr\": 1e-4,\n",
    "    \"learner_l2\": 1e-3, \"adversary_l2\": 1e-4,\n",
    "    \"adversary_norm_reg\": 1e-3,\n",
    "    \"device\": DEVICE,\n",
    "}\n",
    "dml_agmm = DML_mediated(\n",
    "    Y, D, M, W, Z, X,\n",
    "    estimator=\"MR\",\n",
    "    estimand=\"E[Y(1,M(0))]\",\n",
    "    nn_1=[True, True],         # use torch path for both bridge stages\n",
    "    nn_q1=[True, True],        # and for q-models\n",
    "    model1=[m1, m2],\n",
    "    modelq1=[m2, m1],          # your original ordering\n",
    "    n_folds=5, n_rep=1,\n",
    "    fitargs1=[fitargs_seq, fitargs_seq],\n",
    "    fitargsq1=[fitargs_seq, fitargs_seq],\n",
    "    opts={\"lin_degree\": 1, \"burnin\": 200},\n",
    ")\n",
    "t0 = time.perf_counter()\n",
    "res_seq = dml_agmm.dml()\n",
    "t1 = time.perf_counter()\n",
    "summarize_dml_result(\"Sequential (MR) with AGMM\", res_seq, t1 - t0)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) Simultaneous estimator (MR) with AGMM2L2\n",
    "# =========================================================\n",
    "agmm2_model_1  = build_agmm2_for_mediated(M, X, W, Z)\n",
    "agmm2_model_q1 = build_agmm2_for_mediated_q1(M, X, W, Z)\n",
    "\n",
    "fitargs_sim = {\n",
    "    \"n_epochs\": 600, \"bs\": 100,\n",
    "    \"learner_lr\": 1e-4, \"adversary_lr\": 1e-4,\n",
    "    \"learner_l2\": 1e-3, \"adversary_l2\": 1e-4,\n",
    "    \"device\": DEVICE,\n",
    "}\n",
    "opts_sim = {\"burnin\": 400}\n",
    "\n",
    "\n",
    "dml2_agmm = DML_mediated(\n",
    "    Y, D, M, W, Z, X,\n",
    "    estimator=\"MR\",\n",
    "    estimand=\"E[Y(1,M(0))]\",\n",
    "    model1=agmm2_model_1, nn_1=True,\n",
    "    modelq1=agmm2_model_q1, nn_q1=True,\n",
    "    fitargs1=fitargs_sim,\n",
    "    fitargsq1=fitargs_sim,\n",
    "    n_folds=5, n_rep=1, opts=opts_sim,\n",
    ")\n",
    "t0 = time.perf_counter()\n",
    "res_sim = dml2_agmm.dml()\n",
    "t1 = time.perf_counter()\n",
    "summarize_dml_result(\"Simultaneous (MR) with AGMM2L2\", res_sim, t1 - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpiv_venv (3.9.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
