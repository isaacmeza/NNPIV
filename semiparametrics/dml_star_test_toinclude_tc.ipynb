{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, LassoCV, LogisticRegression, LogisticRegressionCV, LinearRegression,\\\n",
    "    ElasticNet, ElasticNetCV, MultiTaskElasticNet, MultiTaskElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mliv.dgps_mediated as dgps\n",
    "from mliv.rkhs import ApproxRKHSIVCV, RKHSIVCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "#from mliv.neuralnet.deepiv_fit import deep_iv_fit\n",
    "from mliv.tsls import tsls, regtsls\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dml_longterm import DML_longterm\n",
    "from dml_npiv import DML_npiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_star.csv', header=0)\n",
    "\n",
    "# Bind covariates\n",
    "Xnan = df[[\"z_score_prior\"]].values\n",
    "X = df[[\"z_score_prior\"]].values\n",
    "G = df[[\"G\"]].values\n",
    "D = df[[\"D\"]].values\n",
    "S = df[[\"z_score3\"]].values\n",
    "V = df[[\"z_score_prior\"]].values\n",
    "Y = df[[\"z_score8\"]].values\n",
    "\n",
    "\n",
    "#drop NaNs\n",
    "X = X[~np.isnan(Xnan)].reshape(-1,1)\n",
    "G = G[~np.isnan(Xnan)].reshape(-1,1)\n",
    "D = D[~np.isnan(Xnan)].reshape(-1,1)\n",
    "S = S[~np.isnan(Xnan)].reshape(-1,1)\n",
    "Y = Y[~np.isnan(Xnan)].reshape(-1,1)\n",
    "V = V[~np.isnan(Xnan)].reshape(-1,1)\n",
    "\n",
    "ones = np.ones((X.shape[0], 1))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dml_2sls = DML_longterm(Y=Y, D=D, S=S, G=G, X1=X,\n",
    "                        estimator='MR',\n",
    "                        longterm_model='latent_unconfounded',\n",
    "                        model1 = ApproxRKHSIVCV(kernel_approx='nystrom', n_components=1000,\n",
    "                           kernel='rbf', gamma=.1, delta_scale='auto',\n",
    "                           delta_exp=.4, alpha_scales=np.geomspace(1, 10000, 10), cv=10),\n",
    "                        model2 = ApproxRKHSIVCV(kernel_approx='nystrom', n_components=200,\n",
    "                           kernel='rbf', gamma=.1, delta_scale='auto',\n",
    "                           delta_exp=.4, alpha_scales=np.geomspace(1, 10000, 10), cv=10),\n",
    "                n_folds=10, n_rep=1, prop_score=LogisticRegression(max_iter=1000))\n",
    "\n",
    "\n",
    "\n",
    "delta_d1, delta_d0, nu_1, nu_0 = dml_2sls._nnpivfit_outcome_latent(Y=Y, D=D, S=S, X=X, G=G)\n",
    "\n",
    "\n",
    "pr_d1_g0_x, pr_g1_d1_sx, pr_g1_d0_sx, pr_g1_x, _ = dml_2sls._propensity_score_latent(S_train=S, X_train=X, D_train=D, G_train=G,\n",
    "                           S_test=S, X_test=X)\n",
    "\n",
    "\n",
    "#Percentile 5 to 95 of X\n",
    "X_test = np.linspace(np.percentile(X, 5), np.percentile(X, 95), 100)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "S_test = S.mean()*np.ones(X_test.shape[0]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "delta_d1_hat = delta_d1.predict(np.column_stack((S, X)))\n",
    "delta_d0_hat = delta_d0.predict(np.column_stack((S, X)))\n",
    "\n",
    "nu_1_hat = nu_1.predict(X)\n",
    "nu_0_hat = nu_0.predict(X)\n",
    "   \n",
    "\n",
    "alfa_1_hat = (G * D * (1-pr_g1_d1_sx)) / (pr_g1_d1_sx * pr_d1_g0_x * (1-pr_g1_x))\n",
    "alfa_0_hat = (G * (1-D) * (1-pr_g1_d0_sx)) / (pr_g1_d0_sx * (1-pr_d1_g0_x) * (1-pr_g1_x))\n",
    "\n",
    "#IPW to residuals of approximation of second outcome bridge\n",
    "eta_1_hat = ((1-G) * D ) / (pr_d1_g0_x * (1-pr_g1_x))\n",
    "eta_0_hat = ((1-G) * (1-D) ) / ((1-pr_d1_g0_x) * (1-pr_g1_x))\n",
    "\n",
    "\n",
    "y1_hat = nu_1_hat + alfa_1_hat * (Y - delta_d1_hat) + eta_1_hat * (delta_d1_hat - nu_1_hat)\n",
    "y0_hat = nu_0_hat + alfa_0_hat * (Y - delta_d0_hat) + eta_0_hat * (delta_d0_hat - nu_0_hat)\n",
    "psi_hat = y1_hat - y0_hat\n",
    "\n",
    "\n",
    "plt.scatter(X,psi_hat)\n",
    "\n",
    "print(psi_hat.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hlo = LinearRegression().fit(X,psi_hat)\n",
    "\n",
    "\n",
    "print(hlo.coef_)\n",
    "print(hlo.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Calculate W.hat using logistic regression from LogisticRegression\n",
    "logit = LogisticRegression().fit(X, D.flatten())\n",
    "D_hat = logit.predict(X)\n",
    "# Build a model for E(Y|X) using linear regression from LinearRegression\n",
    "linreg = LinearRegression().fit(X, Y.flatten())\n",
    "Y_hat = linreg.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = Y.flatten()-Y_hat\n",
    "mean_forest_prediction = (D.flatten() - D_hat) * np.mean(psi_hat)\n",
    "differential_forest_prediction = (D.flatten() - D_hat) * (psi_hat.flatten() - np.mean(psi_hat))\n",
    "\n",
    "#Linear regression with no constant\n",
    "\n",
    "\n",
    "LinearRegression(fit_intercept =False).fit(np.column_stack([mean_forest_prediction,differential_forest_prediction]), target).coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "TC = pd.DataFrame({\n",
    "    'target': Y.flatten()-Y_hat,\n",
    "    'mean.forest.prediction': (D.flatten() - D_hat) * np.mean(psi_hat),\n",
    "    'differential.forest.prediction': (D.flatten() - D_hat) * (psi_hat.flatten() - np.mean(psi_hat))\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the linear model\n",
    "XX = TC[['mean.forest.prediction', 'differential.forest.prediction']]\n",
    "yy = TC['target']\n",
    "model = sm.OLS(yy, XX).fit()\n",
    "\n",
    "# Get the summary of the linear model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = np.arange(5, 100, 5)\n",
    "#Input the vector little v where wwe want to center local estimate theta\n",
    "v_values = np.percentile(V, percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dml_2sls = DML_longterm(Y, D, S, G, X1=None, V=X, v_values = v_values,\n",
    "                        estimator='MR',\n",
    "                        loc_kernel='gau',\n",
    "                        bw_loc='silverman',\n",
    "                        longterm_model='surrogacy',\n",
    "                        model1 = ApproxRKHSIVCV(kernel_approx='nystrom', n_components=200,\n",
    "                                   kernel='rbf', gamma=.1, delta_scale='auto',\n",
    "                                   delta_exp=.4, alpha_scales=np.geomspace(1, 10000, 10), cv=10),\n",
    "                        model2 = ApproxRKHSIVCV(kernel_approx='nystrom', n_components=200,\n",
    "                                   kernel='rbf', gamma=.1, delta_scale='auto',\n",
    "                                   delta_exp=.4, alpha_scales=np.geomspace(1, 10000, 10), cv=10),\n",
    "                n_folds=10, n_rep=1, CHIM = True, prop_score=LogisticRegression(max_iter=2000), opts = {'lin_degree': 3})\n",
    "\n",
    "theta, vart, ci = dml_2sls.dml()\n",
    "\n",
    "lower_ci = ci[:, 0]\n",
    "upper_ci = ci[:, 1]\n",
    "yerr = [theta - lower_ci, upper_ci - theta]\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(theta)\n",
    "plt.axhline(np.mean(theta, axis=0), linewidth=1)  # Adjust line properties as needed\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=1)  # Adjust line properties as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
